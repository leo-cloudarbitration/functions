# -*- coding: utf-8 -*-
"""
Google Sheets ‚Üí BigQuery - SNAPSHOT DE P√ÅGINAS POR HORA
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Sincroniza dados de p√°ginas por hora do Google Sheets para BigQuery:
üöÄ Criado via Cursor - Deploy autom√°tico funcionando!
üìÅ Estrutura: helper/cloud_helper_pages_per_hour/ ‚úÖ

Busca dados do Google Sheets e salva em:
- BigQuery: cloud_snapshot_page_per_hour

SUBSTITUI os dados no BigQuery (WRITE_TRUNCATE) para manter sincronizado
"""

import os
import logging
import pandas as pd
from datetime import datetime
from pytz import timezone
from google.cloud import bigquery
from google.oauth2 import service_account

# Google Sheets
try:
    import gspread
    from oauth2client.service_account import ServiceAccountCredentials
    GSPREAD_AVAILABLE = True
except ImportError:
    GSPREAD_AVAILABLE = False
    logging.warning("Biblioteca gspread n√£o dispon√≠vel. Instale com: pip install gspread oauth2client")

# ------------------------------------------------------------------------------
# CONFIGURA√á√ïES
# ------------------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    handlers=[logging.StreamHandler()]
)

logger = logging.getLogger(__name__)

# Configura√ß√µes Google Sheets
SHEETS_ID = "1hEKsS5VtOw58OKnO6clcSjtZ25ckm5urJSC5EcIV_Oo"
SHEETS_RANGE = "Sheet1"  # Ajuste se necess√°rio

# Configura√ß√µes BigQuery
BIGQUERY_PROJECT = "data-v1-423414"
BIGQUERY_DATASET = "test"
BIGQUERY_TABLE = "cloud_helper_page_per_hour"
TABLE_ID = f"{BIGQUERY_PROJECT}.{BIGQUERY_DATASET}.{BIGQUERY_TABLE}"

# ------------------------------------------------------------------------------
# BIGQUERY CLIENT
# ------------------------------------------------------------------------------
bq_client = None

def get_bq_client():
    """Lazy loading do cliente BigQuery para evitar problemas de inicializa√ß√£o."""
    global bq_client
    if bq_client is None:
        try:
            from google.cloud import bigquery
            from google.oauth2 import service_account
            
            # Verificar se estamos no GitHub Actions (vari√°vel de ambiente GOOGLE_APPLICATION_CREDENTIALS)
            if os.getenv("GOOGLE_APPLICATION_CREDENTIALS"):
                # GitHub Actions: usar credenciais do ambiente
                logger.info("Usando credenciais do GitHub Actions via arquivo JSON")
                bq_client = bigquery.Client(project=BIGQUERY_PROJECT)
            else:
                # Cloud Function: usar Application Default Credentials
                logger.info("Usando Application Default Credentials para Cloud Function")
                bq_client = bigquery.Client(project=BIGQUERY_PROJECT)
            
            logger.info("‚úÖ BigQuery client configurado com sucesso!")
        except Exception as e:
            logger.error("‚ùå Erro ao configurar BigQuery client: %s", str(e))
            bq_client = None
    return bq_client

# ------------------------------------------------------------------------------
# GOOGLE SHEETS CLIENT
# ------------------------------------------------------------------------------
def get_sheets_client():
    """Cria e retorna cliente Google Sheets."""
    if not GSPREAD_AVAILABLE:
        raise ImportError("Biblioteca gspread n√£o est√° instalada")
    
    try:
        # Define o escopo
        scope = [
            'https://spreadsheets.google.com/feeds',
            'https://www.googleapis.com/auth/drive'
        ]
        
        # Verificar se estamos no GitHub Actions
        if os.getenv("GOOGLE_APPLICATION_CREDENTIALS"):
            # GitHub Actions: usar credenciais do arquivo JSON
            creds_file = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
            logger.info(f"üîç Usando credenciais do arquivo: {creds_file}")
            creds = ServiceAccountCredentials.from_json_keyfile_name(creds_file, scope)
        else:
            # Cloud Function: criar credenciais do ambiente
            logger.info("üîç Usando credenciais do ambiente")
            # Voc√™ pode precisar ajustar isso dependendo de como as credenciais s√£o fornecidas
            creds = ServiceAccountCredentials.from_json_keyfile_name(
                os.getenv("GOOGLE_APPLICATION_CREDENTIALS", "credentials.json"), 
                scope
            )
        
        client = gspread.authorize(creds)
        logger.info("‚úÖ Google Sheets client configurado com sucesso!")
        return client
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao configurar Google Sheets client: {e}")
        logger.error(f"‚ùå Tipo do erro: {type(e).__name__}")
        raise

# ------------------------------------------------------------------------------
# FUN√á√ïES PRINCIPAIS
# ------------------------------------------------------------------------------
def fetch_data_from_sheets():
    """
    Busca dados do Google Sheets.
    """
    try:
        logger.info(f"üîç Buscando dados do Google Sheets: {SHEETS_ID}...")
        
        client = get_sheets_client()
        
        # Abrir a planilha
        sheet = client.open_by_key(SHEETS_ID)
        worksheet = sheet.get_worksheet(0)  # Primeira aba
        
        # Buscar todos os dados
        data = worksheet.get_all_records()
        
        if not data:
            logger.warning("‚ö†Ô∏è Nenhum dado encontrado no Google Sheets")
            return []
        
        logger.info(f"‚úÖ {len(data)} registros obtidos do Google Sheets")
        return data
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao buscar dados do Google Sheets: {e}")
        raise

def create_pages_table(table_id: str):
    """Cria a tabela no BigQuery se n√£o existir."""
    bq_client = get_bq_client()
    if not bq_client:
        logger.error("‚ùå Cliente BigQuery n√£o configurado")
        return False
    
    try:
        # Schema conforme especificado: url, category, category_mae
        schema = [
            bigquery.SchemaField("url", "STRING"),
            bigquery.SchemaField("category", "STRING"),
            bigquery.SchemaField("category_mae", "STRING"),
            bigquery.SchemaField("imported_at", "DATETIME")
        ]
        
        table = bigquery.Table(table_id, schema=schema)
        table = bq_client.create_table(table, exists_ok=True)
        logger.info(f"‚úÖ Tabela {table_id} criada/verificada com sucesso")
        return True
    except Exception as e:
        logger.error(f"‚ùå Erro ao criar tabela {table_id}: {e}")
        return False

def upload_to_bigquery(df: pd.DataFrame, table_id: str):
    """
    Faz upload dos dados para o BigQuery.
    Usa WRITE_TRUNCATE para substituir todos os dados (sincroniza√ß√£o completa).
    """
    from google.cloud import bigquery
    
    logger.info("üîç [DEBUG] Iniciando upload_to_bigquery...")
    logger.info(f"üîç [DEBUG] DataFrame: {df is not None}")
    logger.info(f"üîç [DEBUG] DataFrame vazio: {df.empty if df is not None else 'N/A'}")
    logger.info(f"üîç [DEBUG] Tabela: {table_id}")
    
    bq_client = get_bq_client()
    if df is None or bq_client is None:
        logger.error("‚ùå DataFrame nulo ou BigQuery n√£o configurado.")
        return
    
    if df.empty:
        logger.info("‚ö†Ô∏è DataFrame vazio - nenhum dado para upload")
        return
    
    job_cfg = bigquery.LoadJobConfig(
        write_disposition="WRITE_TRUNCATE",  # Substitui todos os dados
        autodetect=True  # Detecta automaticamente o schema
    )
    
    try:
        logger.info(f"üì§ Enviando {len(df)} registros para {table_id}...")
        job = bq_client.load_table_from_dataframe(df, table_id, job_config=job_cfg)
        job.result()
        logger.info(f"‚úÖ {job.output_rows} registros salvos em {table_id}")
    except Exception as e:
        logger.error(f"‚ùå Erro ao adicionar dados ao BigQuery: {e}")
        raise

def sync_pages_per_hour():
    """
    Fun√ß√£o principal: sincroniza dados do Google Sheets para BigQuery.
    """
    try:
        logger.info("üöÄ Iniciando sincroniza√ß√£o de p√°ginas por hora...")
        
        # 1. Buscar dados do Google Sheets
        data = fetch_data_from_sheets()
        
        if not data:
            logger.warning("‚ö†Ô∏è Nenhum dado para sincronizar")
            return
        
        # 2. Converter para DataFrame
        df = pd.DataFrame(data)
        
        # 3. Adicionar timestamp de importa√ß√£o
        local_tz = timezone("America/Sao_Paulo")
        df["imported_at"] = datetime.now(local_tz)
        
        logger.info(f"üìä DataFrame criado com {len(df)} registros")
        logger.info(f"üìã Colunas: {list(df.columns)}")
        
        # 4. Criar tabela se n√£o existir
        if not create_pages_table(TABLE_ID):
            logger.error("‚ùå Falha ao criar tabela no BigQuery")
            return
        
        # 5. Upload para BigQuery
        upload_to_bigquery(df, TABLE_ID)
        
        logger.info("üéâ Sincroniza√ß√£o conclu√≠da com sucesso!")
        
    except Exception as e:
        logger.error(f"‚ùå Erro durante sincroniza√ß√£o: {e}")
        raise

def main():
    """
    Fun√ß√£o principal para execu√ß√£o via GitHub Actions.
    """
    logger.info("üöÄ Iniciando Pages Per Hour Sync (Google Sheets ‚Üí BigQuery)...")
    
    try:
        sync_pages_per_hour()
        logger.info("‚úÖ Pages Per Hour Sync conclu√≠da com sucesso!")
    except Exception as e:
        logger.error(f"‚ùå Erro durante a execu√ß√£o: {e}")
        raise

if __name__ == "__main__":
    main()

