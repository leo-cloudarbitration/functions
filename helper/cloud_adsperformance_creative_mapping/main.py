# -*- coding: utf-8 -*-
"""
Supabase ‚Üí BigQuery - MAPEAMENTO DE CRIATIVOS
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Sincroniza dados de mapeamento de criativos do Supabase para BigQuery:
üöÄ Editado via Cursor - Deploy autom√°tico funcionando!
üìÅ Estrutura: helper/cloud_adsperformance_creative_mapping/ ‚úÖ

Busca dados da tabela Supabase e salva em:
- BigQuery: cloud_adsperformance_creative_mapping

SUBSTITUI os dados no BigQuery (WRITE_TRUNCATE) para manter sincronizado
"""

import os
import logging
import pandas as pd
from datetime import datetime
from pytz import timezone
from google.cloud import bigquery
from google.oauth2 import service_account

# Supabase
try:
    from supabase import create_client, Client
    SUPABASE_AVAILABLE = True
except ImportError:
    SUPABASE_AVAILABLE = False
    logging.warning("Biblioteca supabase n√£o dispon√≠vel. Instale com: pip install supabase")

# ------------------------------------------------------------------------------
# CONFIGURA√á√ïES
# ------------------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    handlers=[logging.StreamHandler()]
)

logger = logging.getLogger(__name__)

# Configura√ß√µes Supabase (via environment variables)
SUPABASE_URL = os.getenv("SUPABASE_URL", "").strip()  # Remove espa√ßos em branco
SUPABASE_KEY = os.getenv("SUPABASE_KEY", "").strip()  # Remove espa√ßos em branco
SUPABASE_TABLE = "adsperfomance_creative_mapping"  # Nome fixo da tabela no Supabase

# Log de debug das configura√ß√µes (sem expor a chave completa)
if SUPABASE_URL:
    logger.info(f"üîç [CONFIG] SUPABASE_URL configurada: {SUPABASE_URL[:30]}...")
else:
    logger.error("‚ùå [CONFIG] SUPABASE_URL n√£o configurada!")

if SUPABASE_KEY:
    logger.info(f"üîç [CONFIG] SUPABASE_KEY configurada: {SUPABASE_KEY[:20]}... (len={len(SUPABASE_KEY)})")
    if SUPABASE_KEY.startswith("eyJ"):
        logger.warning("‚ö†Ô∏è [CONFIG] Detectada 'anon' key - use a 'service_role' key!")
    elif SUPABASE_KEY.startswith("sb_secret_"):
        logger.info("‚úÖ [CONFIG] Secret key detectada (formato correto)")
    else:
        logger.warning(f"‚ö†Ô∏è [CONFIG] Formato de key desconhecido: {SUPABASE_KEY[:10]}...")
else:
    logger.error("‚ùå [CONFIG] SUPABASE_KEY n√£o configurada!")

# Configura√ß√µes BigQuery
BIGQUERY_PROJECT = "data-v1-423414"
BIGQUERY_DATASET = "test"
BIGQUERY_TABLE = "cloud_adsperformance_creative_mapping"
TABLE_ID = f"{BIGQUERY_PROJECT}.{BIGQUERY_DATASET}.{BIGQUERY_TABLE}"

# ------------------------------------------------------------------------------
# BIGQUERY CLIENT
# ------------------------------------------------------------------------------
bq_client = None

def get_bq_client():
    """Lazy loading do cliente BigQuery para evitar problemas de inicializa√ß√£o."""
    global bq_client
    if bq_client is None:
        try:
            from google.cloud import bigquery
            from google.oauth2 import service_account
            
            # Verificar se estamos no GitHub Actions (vari√°vel de ambiente GOOGLE_APPLICATION_CREDENTIALS)
            if os.getenv("GOOGLE_APPLICATION_CREDENTIALS"):
                # GitHub Actions: usar credenciais do ambiente
                logger.info("Usando credenciais do GitHub Actions via arquivo JSON")
                bq_client = bigquery.Client(project=BIGQUERY_PROJECT)
            else:
                # Cloud Function: usar Application Default Credentials
                logger.info("Usando Application Default Credentials para Cloud Function")
                bq_client = bigquery.Client(project=BIGQUERY_PROJECT)
            
            logger.info("‚úÖ BigQuery client configurado com sucesso!")
        except Exception as e:
            logger.error("‚ùå Erro ao configurar BigQuery client: %s", str(e))
            bq_client = None
    return bq_client

# ------------------------------------------------------------------------------
# SUPABASE CLIENT
# ------------------------------------------------------------------------------
def get_supabase_client() -> Client:
    """Cria e retorna cliente Supabase."""
    if not SUPABASE_AVAILABLE:
        raise ImportError("Biblioteca supabase n√£o est√° instalada")
    
    if not SUPABASE_URL or not SUPABASE_KEY:
        logger.error("‚ùå SUPABASE_URL ou SUPABASE_KEY n√£o configurados")
        logger.error(f"   SUPABASE_URL presente: {bool(SUPABASE_URL)}")
        logger.error(f"   SUPABASE_KEY presente: {bool(SUPABASE_KEY)}")
        raise ValueError("SUPABASE_URL e SUPABASE_KEY devem estar configurados")
    
    try:
        logger.info(f"üîç [DEBUG] Tentando conectar ao Supabase...")
        logger.info(f"üîç [DEBUG] URL: {SUPABASE_URL[:30]}... (truncado)")
        logger.info(f"üîç [DEBUG] Key: {SUPABASE_KEY[:20]}... (truncado)")
        
        supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
        logger.info("‚úÖ Supabase client configurado com sucesso!")
        return supabase
    except Exception as e:
        logger.error(f"‚ùå Erro ao configurar Supabase client: {e}")
        logger.error(f"‚ùå Tipo do erro: {type(e).__name__}")
        raise

# ------------------------------------------------------------------------------
# FUN√á√ïES PRINCIPAIS
# ------------------------------------------------------------------------------
def fetch_creative_mapping_from_supabase():
    """
    Busca dados de mapeamento de criativos do Supabase.
    """
    try:
        logger.info(f"üîç Buscando dados da tabela '{SUPABASE_TABLE}' no Supabase...")
        
        supabase = get_supabase_client()
        
        # Buscar todos os registros da tabela
        response = supabase.table(SUPABASE_TABLE).select("*").execute()
        
        if not response.data:
            logger.warning("‚ö†Ô∏è Nenhum dado encontrado no Supabase")
            return []
        
        logger.info(f"‚úÖ {len(response.data)} registros obtidos do Supabase")
        return response.data
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao buscar dados do Supabase: {e}")
        raise

def create_creative_mapping_table(table_id: str):
    """Cria a tabela no BigQuery se n√£o existir."""
    bq_client = get_bq_client()
    if not bq_client:
        logger.error("‚ùå Cliente BigQuery n√£o configurado")
        return False
    
    try:
        # Schema b√°sico - ajuste conforme a estrutura da sua tabela Supabase
        schema = [
            bigquery.SchemaField("id", "STRING"),
            bigquery.SchemaField("creative_id", "STRING"),
            bigquery.SchemaField("creative_name", "STRING"),
            bigquery.SchemaField("campaign_id", "STRING"),
            bigquery.SchemaField("campaign_name", "STRING"),
            bigquery.SchemaField("ad_account_id", "STRING"),
            bigquery.SchemaField("platform", "STRING"),
            bigquery.SchemaField("created_at", "TIMESTAMP"),
            bigquery.SchemaField("updated_at", "TIMESTAMP"),
            bigquery.SchemaField("imported_at", "DATETIME")
        ]
        
        table = bigquery.Table(table_id, schema=schema)
        table = bq_client.create_table(table, exists_ok=True)
        logger.info(f"‚úÖ Tabela {table_id} criada/verificada com sucesso")
        return True
    except Exception as e:
        logger.error(f"‚ùå Erro ao criar tabela {table_id}: {e}")
        return False

def upload_to_bigquery(df: pd.DataFrame, table_id: str):
    """
    Faz upload dos dados para o BigQuery.
    Usa WRITE_TRUNCATE para substituir todos os dados (sincroniza√ß√£o completa).
    """
    from google.cloud import bigquery
    
    logger.info("üîç [DEBUG] Iniciando upload_to_bigquery...")
    logger.info(f"üîç [DEBUG] DataFrame: {df is not None}")
    logger.info(f"üîç [DEBUG] DataFrame vazio: {df.empty if df is not None else 'N/A'}")
    logger.info(f"üîç [DEBUG] Tabela: {table_id}")
    
    bq_client = get_bq_client()
    if df is None or bq_client is None:
        logger.error("‚ùå DataFrame nulo ou BigQuery n√£o configurado.")
        return
    
    if df.empty:
        logger.info("‚ö†Ô∏è DataFrame vazio - nenhum dado para upload")
        return
    
    # Schema - ajuste conforme necess√°rio
    schema = [
        bigquery.SchemaField("id", "STRING"),
        bigquery.SchemaField("creative_id", "STRING"),
        bigquery.SchemaField("creative_name", "STRING"),
        bigquery.SchemaField("campaign_id", "STRING"),
        bigquery.SchemaField("campaign_name", "STRING"),
        bigquery.SchemaField("ad_account_id", "STRING"),
        bigquery.SchemaField("platform", "STRING"),
        bigquery.SchemaField("created_at", "TIMESTAMP"),
        bigquery.SchemaField("updated_at", "TIMESTAMP"),
        bigquery.SchemaField("imported_at", "DATETIME")
    ]
    
    job_cfg = bigquery.LoadJobConfig(
        write_disposition="WRITE_TRUNCATE",  # Substitui todos os dados
        schema=schema
    )
    
    try:
        logger.info(f"üì§ Enviando {len(df)} registros para {table_id}...")
        job = bq_client.load_table_from_dataframe(df, table_id, job_config=job_cfg)
        job.result()
        logger.info(f"‚úÖ {job.output_rows} registros salvos em {table_id}")
    except Exception as e:
        logger.error(f"‚ùå Erro ao adicionar dados ao BigQuery: {e}")
        raise

def sync_creative_mapping():
    """
    Fun√ß√£o principal: sincroniza dados do Supabase para BigQuery.
    """
    try:
        logger.info("üöÄ Iniciando sincroniza√ß√£o de mapeamento de criativos...")
        
        # 1. Buscar dados do Supabase
        data = fetch_creative_mapping_from_supabase()
        
        if not data:
            logger.warning("‚ö†Ô∏è Nenhum dado para sincronizar")
            return
        
        # 2. Converter para DataFrame
        df = pd.DataFrame(data)
        
        # 3. Adicionar timestamp de importa√ß√£o
        local_tz = timezone("America/Sao_Paulo")
        df["imported_at"] = datetime.now(local_tz)
        
        # 4. Converter timestamps se necess√°rio
        if "created_at" in df.columns:
            df["created_at"] = pd.to_datetime(df["created_at"])
        if "updated_at" in df.columns:
            df["updated_at"] = pd.to_datetime(df["updated_at"])
        
        logger.info(f"üìä DataFrame criado com {len(df)} registros")
        logger.info(f"üìã Colunas: {list(df.columns)}")
        
        # 5. Criar tabela se n√£o existir
        if not create_creative_mapping_table(TABLE_ID):
            logger.error("‚ùå Falha ao criar tabela no BigQuery")
            return
        
        # 6. Upload para BigQuery
        upload_to_bigquery(df, TABLE_ID)
        
        logger.info("üéâ Sincroniza√ß√£o conclu√≠da com sucesso!")
        
    except Exception as e:
        logger.error(f"‚ùå Erro durante sincroniza√ß√£o: {e}")
        raise

def main():
    """
    Fun√ß√£o principal para execu√ß√£o via GitHub Actions.
    """
    logger.info("üöÄ Iniciando Creative Mapping Sync (Supabase ‚Üí BigQuery)...")
    
    try:
        sync_creative_mapping()
        logger.info("‚úÖ Creative Mapping Sync conclu√≠da com sucesso!")
    except Exception as e:
        logger.error(f"‚ùå Erro durante a execu√ß√£o: {e}")
        raise

if __name__ == "__main__":
    main()

