# Facebook Page Performance Per Hour - GitHub Actions

Este projeto coleta dados de performance de p√°ginas do Facebook **agregados por hora** e os envia para o BigQuery usando GitHub Actions.

## Caracter√≠sticas

- ‚ú® **Async/Await**: Implementa√ß√£o totalmente ass√≠ncrona para m√°xima performance
- üöÄ **Processamento Paralelo**: M√∫ltiplos grupos e contas processados simultaneamente
- ‚è±Ô∏è **Dados Hor√°rios**: Coleta de m√©tricas agregadas por intervalo de hora
- üìä **BigQuery**: Upload direto dos dados processados

## Configura√ß√£o

### 1. Secrets do GitHub

Configure o seguinte secret no seu reposit√≥rio GitHub:

- `SECRET_GOOGLE_SERVICE_ACCOUNT`: JSON completo das credenciais do Google Cloud Service Account

**Nota**: As credenciais do BigQuery s√£o carregadas automaticamente da vari√°vel de ambiente `SECRET_GOOGLE_SERVICE_ACCOUNT`. Se n√£o estiver dispon√≠vel, o sistema usar√° as credenciais padr√£o do ambiente.

### 2. Estrutura de Arquivos

```
facebook_ads/cloud_facebook_page_per_hour/
‚îú‚îÄ‚îÄ main.py              # Script principal
‚îú‚îÄ‚îÄ requirements.txt     # Depend√™ncias Python
‚îî‚îÄ‚îÄ README.md           # Este arquivo
```

### 3. Execu√ß√£o

O workflow pode ser executado:
- **Agendado**: Configure no GitHub Actions (ex: diariamente √†s 06:00 BRT)
- **Manual**: Via `workflow_dispatch` no GitHub
- **Local**: Execute `python main.py` para testes locais

### 4. Dados Coletados

O script coleta m√©tricas de campanhas do Facebook de **ontem** com breakdown por hora:
- Impress√µes (agregadas por hora)
- Gastos (agregados por hora)
- Cliques em links (agregados por hora)
- Category (extra√≠da do nome da campanha)
- Account name e ID
- Timestamp de importa√ß√£o

### 5. Destino dos Dados

Os dados s√£o enviados para a tabela BigQuery:
`data-v1-423414.test.cloud_facebook_page_per_hour`

### 6. Grupos de Contas

O script processa m√∫ltiplos grupos de contas Facebook em paralelo:
- **CASF A, A2**: 15 contas
- **CASF B, B2**: 34 contas
- **CASF C, C2, C3**: 41 contas
- **Cloud Arbitration 1, 2, 3**: 9 contas
- **CAAG A, B**: 6 contas
- **Nassovia A**: 3 contas

**Total**: 108 contas em 14 grupos

### 7. Performance

- Processamento ass√≠ncrono com `aiohttp`
- Conex√µes simult√¢neas otimizadas (100 total, 30 por host)
- Processamento em lotes de 5 contas por vez
- Rate limiting autom√°tico com delays configur√°veis

### 8. Logs

Todos os logs s√£o exibidos durante a execu√ß√£o, incluindo:
- Status de cada grupo processado
- N√∫mero de registros coletados por grupo
- Total de dados agregados
- Tempo de execu√ß√£o
- Erros e avisos

## Processamento de Dados

### Extra√ß√£o de Category
A category √© extra√≠da do nome da campanha:
- Extrai os primeiros 3 segmentos separados por "_"
- Exemplo: `CASF_BR_GOLD_...` ‚Üí Category: `CASF_BR_GOLD`

### Agrega√ß√£o
Os dados s√£o agregados por:
- Category
- Account name
- Account ID
- Date
- Time interval (hora)

## Troubleshooting

### Erro de Credenciais
- Verifique se o secret `SECRET_GOOGLE_SERVICE_ACCOUNT` est√° configurado corretamente
- Confirme se o Service Account tem permiss√µes no BigQuery

### Rate Limiting
- O script j√° inclui delays autom√°ticos para evitar rate limiting
- Configura√ß√µes atuais:
  - `REQUEST_DELAY`: 0.35s
  - `ACCOUNT_DELAY`: 0.7s
  - `RATE_LIMIT_DELAY`: 30.0s
  - Batch size: 5 contas por vez

### Timeout
- Timeout configurado para 60 segundos por requisi√ß√£o
- Se necess√°rio, ajuste em `aiohttp.ClientTimeout(total=60)`

### Dados Excessivos
- Algumas contas podem retornar erro 500 por dados excessivos
- O script tenta novamente automaticamente com exponential backoff (3 tentativas)

## Depend√™ncias Principais

- `aiohttp`: Cliente HTTP ass√≠ncrono
- `pandas`: Processamento de dados
- `google-cloud-bigquery`: Upload para BigQuery
- `functions-framework`: Suporte para Cloud Functions
- `pytz`: Timezone handling

## Adapta√ß√µes Necess√°rias

Antes de fazer deploy em produ√ß√£o, considere:

1. **Tokens de Acesso Facebook**: ‚ö†Ô∏è Os tokens do Facebook ainda est√£o hardcoded no arquivo. Considere mov√™-los para vari√°veis de ambiente ou secrets do GitHub para maior seguran√ßa
2. **Credenciais GCP**: ‚úÖ J√° configurado para usar secrets do GitHub via `SECRET_GOOGLE_SERVICE_ACCOUNT`
3. **Workflow File**: Criar `.github/workflows/cloud_facebook_page_per_hour.yml`
4. **Table ID**: Validar a tabela de destino no BigQuery
5. **Agendamento**: Configurar hor√°rio ideal de execu√ß√£o

## Pr√≥ximos Passos

- [ ] Criar workflow do GitHub Actions
- [ ] Mover credenciais para secrets
- [ ] Configurar agendamento autom√°tico
- [ ] Testar execu√ß√£o completa
- [ ] Documentar estrutura da tabela BigQuery

