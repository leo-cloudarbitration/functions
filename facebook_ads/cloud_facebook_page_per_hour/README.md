# Facebook Page Performance Per Hour - GitHub Actions

Este projeto coleta dados de performance de p√°ginas do Facebook **agregados por hora** e os envia para o BigQuery usando GitHub Actions.

## Caracter√≠sticas

- ‚ú® **Async/Await**: Implementa√ß√£o totalmente ass√≠ncrona para m√°xima performance
- üöÄ **Processamento Paralelo**: M√∫ltiplos grupos e contas processados simultaneamente
- ‚è±Ô∏è **Dados Hor√°rios**: Coleta de m√©tricas agregadas por intervalo de hora
- üìä **BigQuery**: Upload direto dos dados processados

## Configura√ß√£o

### 1. Secrets do GitHub

Configure o seguinte secret no seu reposit√≥rio GitHub:

- `SECRET_GOOGLE_SERVICE_ACCOUNT`: JSON completo das credenciais do Google Cloud Service Account

**Nota**: As credenciais do BigQuery s√£o carregadas automaticamente da vari√°vel de ambiente `SECRET_GOOGLE_SERVICE_ACCOUNT`. Se n√£o estiver dispon√≠vel, o sistema usar√° as credenciais padr√£o do ambiente.

### 2. Estrutura de Arquivos

```
facebook_ads/cloud_facebook_page_per_hour/
‚îú‚îÄ‚îÄ main.py              # Script principal
‚îú‚îÄ‚îÄ requirements.txt     # Depend√™ncias Python
‚îî‚îÄ‚îÄ README.md           # Este arquivo
```

### 3. Execu√ß√£o

O workflow √© executado automaticamente:
- **Agendado**: Todos os dias √†s 10:00 BRT (13:00 UTC)
- **Manual**: Via `workflow_dispatch` no GitHub
- **Local**: Execute `python main.py` para testes locais

### 4. Workflow GitHub Actions

O arquivo `.github/workflows/cloud_facebook_page_per_hour.yml` est√° configurado para:
- Rodar automaticamente todos os dias √†s 10h da manh√£ (hor√°rio de Bras√≠lia)
- Pode ser executado manualmente via GitHub Actions
- Autentica automaticamente com Google Cloud usando secrets

### 4. Dados Coletados

O script coleta m√©tricas de campanhas do Facebook de **ontem** com breakdown por hora:
- Impress√µes (agregadas por hora)
- Gastos (agregados por hora)
- Cliques em links (agregados por hora)
- Category (extra√≠da do nome da campanha)
- Account name e ID
- Timestamp de importa√ß√£o

### 5. Destino dos Dados

Os dados s√£o enviados para a tabela BigQuery:
`data-v1-423414.test.cloud_facebook_page_per_hour`

**Modo de escrita**: `WRITE_APPEND` - os dados s√£o acumulados na tabela, permitindo hist√≥rico completo

### 6. Grupos de Contas

O script processa m√∫ltiplos grupos de contas Facebook em paralelo:
- **CASF A, A2**: 15 contas
- **CASF B, B2**: 34 contas
- **CASF C, C2, C3**: 41 contas
- **Cloud Arbitration 1, 2, 3**: 9 contas
- **CAAG A, B**: 6 contas
- **Nassovia A**: 3 contas

**Total**: 108 contas em 14 grupos

### 7. Performance

- Processamento ass√≠ncrono com `aiohttp`
- Conex√µes simult√¢neas otimizadas (100 total, 30 por host)
- Processamento em lotes de 5 contas por vez
- Rate limiting autom√°tico com delays configur√°veis

### 8. Logs

Todos os logs s√£o exibidos durante a execu√ß√£o, incluindo:
- Status de cada grupo processado
- N√∫mero de registros coletados por grupo
- Total de dados agregados
- Tempo de execu√ß√£o
- Erros e avisos

## Processamento de Dados

### Extra√ß√£o de Category
A category √© extra√≠da do nome da campanha:
- Extrai os primeiros 3 segmentos separados por "_"
- Exemplo: `CASF_BR_GOLD_...` ‚Üí Category: `CASF_BR_GOLD`

### Agrega√ß√£o
Os dados s√£o agregados por:
- Category
- Account name
- Account ID
- Date
- Time interval (hora)

## Troubleshooting

### Erro de Credenciais
- Verifique se o secret `SECRET_GOOGLE_SERVICE_ACCOUNT` est√° configurado corretamente
- Confirme se o Service Account tem permiss√µes no BigQuery

### Rate Limiting
- O script j√° inclui delays autom√°ticos para evitar rate limiting
- Configura√ß√µes atuais:
  - `REQUEST_DELAY`: 0.35s
  - `ACCOUNT_DELAY`: 0.7s
  - `RATE_LIMIT_DELAY`: 30.0s
  - Batch size: 5 contas por vez

### Timeout
- Timeout configurado para 60 segundos por requisi√ß√£o
- Se necess√°rio, ajuste em `aiohttp.ClientTimeout(total=60)`

### Dados Excessivos
- Algumas contas podem retornar erro 500 por dados excessivos
- O script tenta novamente automaticamente com exponential backoff (3 tentativas)

## Depend√™ncias Principais

- `aiohttp`: Cliente HTTP ass√≠ncrono
- `pandas`: Processamento de dados
- `google-cloud-bigquery`: Upload para BigQuery
- `functions-framework`: Suporte para Cloud Functions
- `pytz`: Timezone handling

## Configura√ß√£o para Produ√ß√£o

### ‚úÖ J√° Configurado

1. **Credenciais GCP**: ‚úÖ Configurado para usar secrets do GitHub via `SECRET_GOOGLE_SERVICE_ACCOUNT`
2. **Workflow GitHub Actions**: ‚úÖ Criado e configurado para rodar √†s 10h BRT
3. **Modo de Escrita**: ‚úÖ WRITE_APPEND ativado para acumular hist√≥rico
4. **Processamento Ass√≠ncrono**: ‚úÖ Otimizado para m√°xima performance

### ‚ö†Ô∏è Opcional

1. **Tokens de Acesso Facebook**: Os tokens do Facebook est√£o hardcoded no arquivo. Considere mov√™-los para vari√°veis de ambiente ou secrets do GitHub para maior seguran√ßa
2. **Table ID**: Validar se a tabela de destino no BigQuery est√° correta

## Status do Projeto

- [x] Criar workflow do GitHub Actions ‚úÖ
- [x] Configurar agendamento autom√°tico (10h BRT diariamente) ‚úÖ
- [x] Testar execu√ß√£o completa ‚úÖ
- [x] Configurar WRITE_APPEND para acumular hist√≥rico ‚úÖ
- [ ] Mover tokens do Facebook para secrets (opcional)
- [ ] Documentar estrutura da tabela BigQuery (opcional)

